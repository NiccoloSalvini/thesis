<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 6 Bayesian Spatial Modelling | Spatial Machine Learning modelling: End-to-End web app solution</title>
  <meta name="description" content="Chapter 6 Bayesian Spatial Modelling | Spatial Machine Learning modelling: End-to-End web app solution by Niccol√≤ Salvini" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 6 Bayesian Spatial Modelling | Spatial Machine Learning modelling: End-to-End web app solution" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://niccolosalvini.github.io/Thesis/" />
  <meta property="og:image" content="https://niccolosalvini.github.io/Thesis/images/spat-touch.png" />
  <meta property="og:description" content="Chapter 6 Bayesian Spatial Modelling | Spatial Machine Learning modelling: End-to-End web app solution by Niccol√≤ Salvini" />
  <meta name="github-repo" content="NiccoloSalvini/Thesis" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 6 Bayesian Spatial Modelling | Spatial Machine Learning modelling: End-to-End web app solution" />
  
  <meta name="twitter:description" content="Chapter 6 Bayesian Spatial Modelling | Spatial Machine Learning modelling: End-to-End web app solution by Niccol√≤ Salvini" />
  <meta name="twitter:image" content="https://niccolosalvini.github.io/Thesis/images/spat-touch.png" />

<meta name="author" content="Niccol√≤ Salvini" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  <link rel="apple-touch-icon-precomposed" sizes="120x120" href="images/spatial.png" />
  <link rel="shortcut icon" href="images/favicon.ico" type="image/x-icon" />
<link rel="prev" href="prd.html"/>
<link rel="next" href="applications.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-171723874-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-171723874-1');
</script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="css\style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Spatial Machine Learning Modelling: End-to-End web App solution</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preliminary Content</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#preface"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#dedication"><i class="fa fa-check"></i>Dedication</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#abstract"><i class="fa fa-check"></i>Abstract</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="scraping.html"><a href="scraping.html"><i class="fa fa-check"></i><b>2</b> Scraping</a><ul>
<li class="chapter" data-level="2.1" data-path="scraping.html"><a href="scraping.html#what-is-scraping"><i class="fa fa-check"></i><b>2.1</b> What is Scraping</a></li>
<li class="chapter" data-level="2.2" data-path="scraping.html"><a href="scraping.html#scraping-best-practices-and-robot.txt"><i class="fa fa-check"></i><b>2.2</b> Scraping Best Practices and Robot.txt</a></li>
<li class="chapter" data-level="2.3" data-path="scraping.html"><a href="scraping.html#user-agents-proxies-handlers"><i class="fa fa-check"></i><b>2.3</b> User agents, Proxies, Handlers</a><ul>
<li class="chapter" data-level="2.3.1" data-path="scraping.html"><a href="scraping.html#user-agents-spoofing"><i class="fa fa-check"></i><b>2.3.1</b> User agents Spoofing</a></li>
<li class="chapter" data-level="2.3.2" data-path="scraping.html"><a href="scraping.html#handlers"><i class="fa fa-check"></i><b>2.3.2</b> Handlers</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="scraping.html"><a href="scraping.html#how-they-are-designed-with-rvest"><i class="fa fa-check"></i><b>2.4</b> How they are designed with <code>rvest</code></a><ul>
<li class="chapter" data-level="2.4.1" data-path="scraping.html"><a href="scraping.html#from-generic-and-specific-structure"><i class="fa fa-check"></i><b>2.4.1</b> From Generic and Specific structure</a></li>
<li class="chapter" data-level="2.4.2" data-path="scraping.html"><a href="scraping.html#parallel-computing"><i class="fa fa-check"></i><b>2.4.2</b> Parallel Computing</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="scraping.html"><a href="scraping.html#what-are-the-advantages-of-this-workflow"><i class="fa fa-check"></i><b>2.5</b> What are the Advantages of this Workflow</a></li>
<li class="chapter" data-level="2.6" data-path="scraping.html"><a href="scraping.html#legal-challenges-ancora-non-validato"><i class="fa fa-check"></i><b>2.6</b> Legal Challenges (ancora non validato)</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="Infrastructure.html"><a href="Infrastructure.html"><i class="fa fa-check"></i><b>3</b> Infrastructure</a><ul>
<li class="chapter" data-level="3.1" data-path="Infrastructure.html"><a href="Infrastructure.html#scheduler"><i class="fa fa-check"></i><b>3.1</b> Scheduler</a><ul>
<li class="chapter" data-level="3.1.1" data-path="Infrastructure.html"><a href="Infrastructure.html#cron-jobs"><i class="fa fa-check"></i><b>3.1.1</b> Cron Jobs</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="Infrastructure.html"><a href="Infrastructure.html#docker-container"><i class="fa fa-check"></i><b>3.2</b> Docker Container</a><ul>
<li class="chapter" data-level="3.2.1" data-path="Infrastructure.html"><a href="Infrastructure.html#what-is-docker"><i class="fa fa-check"></i><b>3.2.1</b> What is Docker?</a></li>
<li class="chapter" data-level="3.2.2" data-path="Infrastructure.html"><a href="Infrastructure.html#what-are-the-main-andvantages-of-using-docker"><i class="fa fa-check"></i><b>3.2.2</b> What are the main andvantages of using Docker</a></li>
<li class="chapter" data-level="3.2.3" data-path="Infrastructure.html"><a href="Infrastructure.html#dockerfile"><i class="fa fa-check"></i><b>3.2.3</b> Dockerfile</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="Infrastructure.html"><a href="Infrastructure.html#api"><i class="fa fa-check"></i><b>3.3</b> API</a></li>
<li class="chapter" data-level="3.4" data-path="Infrastructure.html"><a href="Infrastructure.html#what-is-an-api"><i class="fa fa-check"></i><b>3.4</b> What is an API</a><ul>
<li class="chapter" data-level="3.4.1" data-path="Infrastructure.html"><a href="Infrastructure.html#what-in-practice-an-api-does"><i class="fa fa-check"></i><b>3.4.1</b> What in practice an API does</a></li>
<li class="chapter" data-level="3.4.2" data-path="Infrastructure.html"><a href="Infrastructure.html#plumber-api"><i class="fa fa-check"></i><b>3.4.2</b> Plumber API</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="spatial.html"><a href="spatial.html"><i class="fa fa-check"></i><b>4</b> Spatial Statistics</a><ul>
<li class="chapter" data-level="4.1" data-path="spatial.html"><a href="spatial.html#gentle-introduction"><i class="fa fa-check"></i><b>4.1</b> Gentle Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="spatial.html"><a href="spatial.html#inla-estimation"><i class="fa fa-check"></i><b>4.2</b> INLA estimation</a></li>
<li class="chapter" data-level="4.3" data-path="spatial.html"><a href="spatial.html#presentation-of-data"><i class="fa fa-check"></i><b>4.3</b> Presentation of data</a></li>
<li class="chapter" data-level="4.4" data-path="spatial.html"><a href="spatial.html#point-referenced-data-models"><i class="fa fa-check"></i><b>4.4</b> Point-referenced data models</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="prd.html"><a href="prd.html"><i class="fa fa-check"></i><b>5</b> Point Referenced Data</a><ul>
<li class="chapter" data-level="5.1" data-path="prd.html"><a href="prd.html#point-referenced-modeling"><i class="fa fa-check"></i><b>5.1</b> Point-Referenced modeling</a><ul>
<li class="chapter" data-level="5.1.1" data-path="prd.html"><a href="prd.html#stationarity"><i class="fa fa-check"></i><b>5.1.1</b> Stationarity</a></li>
<li class="chapter" data-level="5.1.2" data-path="prd.html"><a href="prd.html#variograms"><i class="fa fa-check"></i><b>5.1.2</b> Variograms</a></li>
<li class="chapter" data-level="5.1.3" data-path="prd.html"><a href="prd.html#isotropy"><i class="fa fa-check"></i><b>5.1.3</b> Isotropy</a></li>
<li class="chapter" data-level="5.1.4" data-path="prd.html"><a href="prd.html#variogram-model-fitting"><i class="fa fa-check"></i><b>5.1.4</b> Variogram model fitting</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="prd.html"><a href="prd.html#anisotropy"><i class="fa fa-check"></i><b>5.2</b> Anisotropy</a></li>
<li class="chapter" data-level="5.3" data-path="prd.html"><a href="prd.html#exploratory-analysis"><i class="fa fa-check"></i><b>5.3</b> Exploratory analysis</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="bayesianspat.html"><a href="bayesianspat.html"><i class="fa fa-check"></i><b>6</b> Bayesian Spatial Modelling</a><ul>
<li class="chapter" data-level="6.1" data-path="bayesianspat.html"><a href="bayesianspat.html#inla"><i class="fa fa-check"></i><b>6.1</b> INLA</a></li>
<li class="chapter" data-level="6.2" data-path="bayesianspat.html"><a href="bayesianspat.html#laplace-approximation"><i class="fa fa-check"></i><b>6.2</b> Laplace Approximation</a></li>
<li class="chapter" data-level="6.3" data-path="bayesianspat.html"><a href="bayesianspat.html#the-class-of-latent-gaussian-models"><i class="fa fa-check"></i><b>6.3</b> The Class of Latent Gaussian Models</a><ul>
<li class="chapter" data-level="6.3.1" data-path="bayesianspat.html"><a href="bayesianspat.html#approximate-bayesian-inference-with-inla"><i class="fa fa-check"></i><b>6.3.1</b> Approximate Bayesian inference with INLA</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="applications.html"><a href="applications.html"><i class="fa fa-check"></i><b>7</b> Applications</a><ul>
<li class="chapter" data-level="7.1" data-path="applications.html"><a href="applications.html#example-one"><i class="fa fa-check"></i><b>7.1</b> Example one</a></li>
<li class="chapter" data-level="7.2" data-path="applications.html"><a href="applications.html#example-two"><i class="fa fa-check"></i><b>7.2</b> Example two</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="final-words.html"><a href="final-words.html"><i class="fa fa-check"></i><b>8</b> Final Words</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/NiccoloSalvini/tesi-prova" target="blank"> See Github Repository</a></li>
<li><a href="https://niccolosalvini.netlify.app/">About The Author</a></li>
<li><a Proudly published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Spatial Machine Learning modelling: End-to-End web app solution</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="bayesianspat" class="section level1">
<h1><span class="header-section-number">Chapter 6</span> Bayesian Spatial Modelling</h1>
<p><strong>va parafrasatoo</strong></p>
<p>Several types of models are used with spatial and spatio-temporal data, depend-
ing on the aim of the study. If we are interested in summarizing spatial and
spatio-temporal variation between areas using risks or probabilities then we
could use statistical methods like disease mapping to compare maps and identify
clusters. Moran Index is extensively used to check for spatial autocorrelation
(Moran, 1950), while the scan statistics, implemented in SaTScan (Killdorf, 1997),
has been used for cluster detection and to perform geographical surveillance in
a non-Bayesian approach. The same types of models can also be used in studies
where there is an aetiological aim to assess the potential effect of risk factors on
outcomes.
A different type of study considers the quantification of the risk of experienc-
ing an outcome as the distance from a certain source increases. This is typically
framed in an environmental context, so that the source could be a point (e.g., waste
site, radio transmitter) or a line (e.g., power line, road). In this case, the meth-
ods typically used vary from nonparametric tests proposed by Stone (1988) to the
parametric approach introduced by Diggle et al.¬†(1998).
In a different context, when the interest lies in mapping continuous spatial (or
spatio-temporal) variables, which are measured only at a finite set of specific points
in a given region, and in predicting their values at unobserved locations, geostatis-
tical methods ‚Äì such as kriging ‚Äì are employed (Cressie, 1991; Stein, 1991). This
may play a significant role in environmental risk assessment in order to identify
areas where the risk of exceeding potentially harmful thresholds is higher.
Bayesian methods to deal with spatial and spatio-temporal data started to appear
around year 2000, with the development of Markov chain Monte Carlo (MCMC)
simulative methods (Casella and George, 1992; Gilks et al., 1996). Before that the
Bayesian approach was almost only used for theoretical models and found little
applications in real case studies due to the lack of numerical/analytical or simula-
tive tools to compute posterior distributions. The advent of MCMC has triggered
the possibility for researchers to develop complex models on large datasets without
INTRODUCTION
3
the need of imposing simplified structures. Probably the main contribution to spatial
and spatio-temporal statistics is the one of Besag et al.¬†(1991), who developed
the Besag‚ÄìYork‚ÄìMolli√© (BYM) method (see Chapter 6) which is commonly used
for disease mapping, while Banerjee et al.¬†(2004), Diggle and Ribeiro (2007) and
Cressie and Wikle (2011) have concentrated on Bayesian geostatistical models.
The main advantage of the Bayesian approach resides in its taking into account
uncertainty in the estimates/predictions, and its flexibility and capability of dealing
with issues like missing data. In the book, we follow this paradigm and introduce
the Bayesian philosophy and inference in Chapter 3, while in Chapter 4 we review
Bayesian computation tools, but the reader could also find interesting the follow-
ing: Knorr-Held (2000) and Best et al.¬†(2005) for disease mapping and Diggle et al.
(1998) for a modeling approach for continuous spatial data and for prediction</p>
<p><strong>va parafrasatoo</strong></p>
<div id="inla" class="section level2">
<h2><span class="header-section-number">6.1</span> INLA</h2>
<p>For many years, Bayesian inference has relied upon Markov chain Monte Carlo methods (Gilks et al.¬†1996; Brooks et al.¬†2011) to compute the joint posterior distribution of the model parameters. This is usually computationally very expensive as this distribution is often in a space of high dimension.</p>
<p>Havard Rue, Martino, and Chopin (2009) propose a novel approach that makes Bayesian inference faster. First of all, rather than aiming at estimating the joint posterior distribution of the model parameters, they suggest focusing on individual posterior marginals of the model parameters. In many cases, marginal inference is enough to make inference of the model parameters and latent effects, and there is no need to deal with multivariate posterior distributions that are difficult to obtain. Secondly, they focus on models that can be expressed as latent Gaussian Markov random fields (GMRF). This provides the computational advantages (see Rue and Held 2005) that reduce computation time of model fitting. Furthermore, Havard Rue, Martino, and Chopin (2009) develop a new approximation to the posterior marginal distributions of the model parameters based on the Laplace approximation (see, for example, MacKay 2003). A recent review on INLA can be found in Rue et al.¬†(2017).</p>
</div>
<div id="laplace-approximation" class="section level2">
<h2><span class="header-section-number">6.2</span> Laplace Approximation</h2>
<p>An alternative approach to the simulation-based MC integration is analytic approx-
imation with the Laplace method. Suppose we are interested in computing the
following integral:</p>
<p><span class="math display">\[\int f(x) \mathrm{d} x=\int \exp (\log f(x)) \mathrm{d} x\]</span></p>
<p>where <span class="math inline">\(f(x)\)</span> is the density function of a random variable X. We represent <span class="math inline">\(log f(x)\)</span> by
means of a Taylor series expansion evaluated in x = x0:</p>
<p><span class="math display">\[\log f(x) \approx \log f\left(x_{0}\right)+\left.\left(x-x_{0}\right) \frac{\partial \log f(x)}{\partial x}\right|_{x=x_{0}}+\left.\frac{\left(x-x_{0}\right)^{2}}{2} \frac{\partial^{2} \log f(x)}{\partial x^{2}}\right|_{x=x_{0}}\]</span></p>
<p>If x0 is set equal to the mode <span class="math inline">\(x‚àó = argmax\)</span>, log f(x) then log f(x)<span class="math inline">\(\left.\frac{\partial \log f(x)}{\partial x}\right|_{x=x^{*}}=0\)</span> and the approximation becomes</p>
<p><span class="math display">\[\log f(x) \approx \log f\left(x^{*}\right)+\left.\frac{\left(x-x^{*}\right)^{2}}{2} \frac{\partial^{2} \log f(x)}{\partial x^{2}}\right|_{x=x^{*}}\]</span></p>
<p>The integral of interest is then approximated as follows:</p>
<p><span class="math display">\[\int f(x) \mathrm{d} x \approx \int \exp \left(\log f\left(x^{*}\right)+\left.\frac{\left(x-x^{*}\right)^{2}}{2} \frac{\partial^{2} \log f(x)}{\partial x^{2}}\right|_{x=x^{*}}\right) \mathrm{d} x\]</span></p>
<p><span class="math display">\[=\exp \left(\log f\left(x^{*}\right)\right) \int \exp \left(\left.\frac{\left(x-x^{*}\right)^{2}}{2} \frac{\partial^{2} \log f(x)}{\partial x^{2}}\right|_{x=x^{*}}\right) \mathrm{d} x\]</span></p>
<p>where the integrand can be associated with the density of a Normal distribution. In
fact, by setting <span class="math display">\[\sigma^{2 *}=-1 /\left.\frac{\partial^{2} \log f(x)}{\partial x^{2}}\right|_{x=x^{*}}\]</span> we obtain:</p>
<p><span class="math display">\[\int f(x) \mathrm{d} x \approx \exp \left(\log f\left(x^{*}\right)\right) \int \exp \left(-\frac{\left(x-x^{*}\right)^{2}}{2 \sigma^{2 *}}\right) \mathrm{d} x\]</span></p>
<p>where the integrand is the kernel of a Normal distribution with mean equal to x‚àó
and variance <span class="math inline">\(\sigma^{2*}\)</span>. More precisely, the integral evaluated in the interval <span class="math inline">\((\alpha, \beta)\)</span> is
approximated by:</p>
<p><span class="math display">\[\int_{\alpha}^{\beta} f(x) \mathrm{d} x \approx f\left(x^{*}\right) \sqrt{2 \pi \sigma^{2 *}}(\Phi(\beta)-\Phi(\alpha))\]</span></p>
<p>where <span class="math inline">\(\Phi(‚ãÖ)\)</span> denotes the cumulative density function of th <span class="math inline">\(Normal(x_i, \sigma^{2*})\)</span> distri-
bution.</p>
<p><strong>qui volendo un esempio fatto da me</strong></p>
</div>
<div id="the-class-of-latent-gaussian-models" class="section level2">
<h2><span class="header-section-number">6.3</span> The Class of Latent Gaussian Models</h2>
<p>The first step in defining a latent Gaussian model within the Bayesian framework
is to identify a distribution for the observed data y = (y1, ‚Ä¶ , yn). A very general
approach consists in specifying a distribution for yi characterized by a parameter
ùúôi (usually the mean E(yi)) defined as a function of a structured additive predictor
ùúÇi through a link function g(‚ãÖ), such that g(ùúôi) = ùúÇi. The additive linear predictor ùúÇi
is defined as follows:</p>
<p><span class="math display">\[\eta_{i}=\beta_{0}+\sum_{m=1}^{M} \beta_{m} x_{m i}+\sum_{l=1}^{L} f_{l}\left(z_{l i}\right)\]</span></p>
<p>Here ùõΩ0 is a scalar representing the intercept; the coefficients <span class="math inline">\(\boldsymbol{\beta}=\left\{\beta_{1}, \ldots, \beta_{M}\right\}\)</span>
quantify the (linear) effect of some covariates <span class="math inline">\(\boldsymbol{x}=\left(\boldsymbol{x}_{1}, \ldots, \boldsymbol{x}_{M}\right)\)</span> on the response; and
<span class="math inline">\(f=\left\{f_{1}(\cdot), \ldots, f_{L}(\cdot)\right\}\)</span> is a collection of functions defined in terms of a set of covariates <span class="math inline">\(z = (z_1, ‚Ä¶ , z_L)\)</span>. The terms <span class="math inline">\(f_l(‚ãÖ)\)</span> can assume different forms such as smooth and</p>
<p>nonlinear effects of covariates, time trends and seasonal effects, random intercept
and slopes as well as temporal or spatial random effects. For this reason, the class of
latent Gaussian models is very flexible and can accomodate a wide range of models
ranging from generalized and dynamic linear models to spatial and spatio-temporal
models (see Martins et al., 2013 for a review).
We collect all the latent (nonobservable) components of interest for the inference
in a set of parameters named ùúΩ defined as <span class="math inline">\(\boldsymbol{\theta}=\left\{\beta_{0}, \boldsymbol{\beta}, \boldsymbol{f}\right\}\)</span>. Moreover, we denote with
<span class="math inline">\(\psi=\left\{\psi_{1}, \ldots, \psi_{K}\right\}\)</span> the vector of the K hyperparameters. By assuming conditional
independence, the distribution of the n observations (all coming from the same
distribution family) is given by the likelihood</p>
<p><span class="math display">\[p(\boldsymbol{y} \mid \boldsymbol{\theta}, \boldsymbol{\psi})=\prod_{i=1}^{n} p\left(y_{i} \mid \theta_{i}, \boldsymbol{\psi}\right)\]</span></p>
<p>where each data point yi is connected to only one element ùúÉi in the latent field <span class="math inline">\(\theta\)</span>.
Martins et al.¬†(2013) discuss the possibility of relaxing this assumption assuming
that each observation may be connected with a linear combination of elements in
<span class="math inline">\(\theta\)</span>; moreover, they take into account the case when the data belong to several distri-
butions, i.e., the multiple likelihoods case.</p>
<p>We assume a multivariate Normal prior on ùúΩ with mean ùüé and precision matrix
<span class="math inline">\(Q(\psi)\)</span>, i.e., <span class="math inline">\(\boldsymbol{\theta} \sim \operatorname{Normal}\left(\mathbf{0}, \boldsymbol{Q}^{-1}(\boldsymbol{\psi})\right)\)</span> with density function given by</p>
<p><span class="math display">\[p(\theta \mid \psi)=(2 \pi)^{-n / 2}|Q(\psi)|^{1 / 2} \exp \left(-\frac{1}{2} \theta^{\prime} Q(\psi) \theta\right)\]</span></p>
<p>where | ‚ãÖ | denotes the matrix determinant and ‚Ä≤ is used for the transpose operation.
The components of the latent Gaussian field ùúΩ are supposed to be conditionally
independent with the consequence that <span class="math inline">\(Q(\psi)\)</span> is a sparse precision matrix.8 This
specification is known as Gaussian Markov random field (GMRF, Rue and Held,
2005). Note that the sparsity of the precision matrix gives rise to computational ben-
efits when making inference with GMRFs. In fact, linear algebra operations can be
performed using numerical methods for sparse matrices, resulting in a considerable
computational gain (see Rue and Held, 2005 for algorithms).
The joint posterior distribution of ùúΩ and <span class="math inline">\(\psi\)</span> is given by the product of the likelihood
(4.13), of the GMRF density (4.14) and of the hyperparameter prior distribution
<span class="math inline">\(p(\psi)\)</span>:</p>
<p><span class="math display">\[
\begin{aligned} p(\boldsymbol{\theta}, \boldsymbol{\psi} \mid \boldsymbol{y}) &amp; \propto p(\boldsymbol{\psi}) \times p(\boldsymbol{\theta} \mid \boldsymbol{\psi}) \times p(\boldsymbol{y} \mid \boldsymbol{\theta}, \boldsymbol{\psi}) \\ &amp; \propto p(\boldsymbol{\psi}) \times p(\boldsymbol{\theta} \mid \boldsymbol{\psi}) \times \prod_{i=1}^{n} p\left(y_{i} \mid \theta_{i}, \boldsymbol{\psi}\right) \\ &amp; \propto p(\boldsymbol{\psi}) \times|\boldsymbol{Q}(\boldsymbol{\psi})|^{1 / 2} \exp \left(-\frac{1}{2} \boldsymbol{\theta}^{\prime} \boldsymbol{Q}(\boldsymbol{\psi}) \boldsymbol{\theta}\right) \times \prod_{i=1}^{n} \exp \left(\log \left(p\left(y_{i} \mid \theta_{i}, \boldsymbol{\psi}\right)\right)\right) \\ &amp; \propto p(\boldsymbol{\psi}) \times|\boldsymbol{Q}(\boldsymbol{\psi})|^{1 / 2} \exp \left(-\frac{1}{2} \boldsymbol{\theta}^{\prime} \boldsymbol{Q}(\boldsymbol{\psi}) \boldsymbol{\theta}+\sum_{i=1}^{n} \log \left(p\left(y_{i} \mid \theta_{i}, \boldsymbol{\psi}\right)\right)\right) \end{aligned}
\]</span></p>
<div id="approximate-bayesian-inference-with-inla" class="section level3">
<h3><span class="header-section-number">6.3.1</span> Approximate Bayesian inference with INLA</h3>
<p>The objectives of Bayesian inference are the marginal posterior distributions for
each element of the parameter vector</p>
<p><span class="math display">\[p\left(\theta_{i} \mid \boldsymbol{y}\right)=\int p\left(\theta_{i}, \boldsymbol{\psi} \mid \boldsymbol{y}\right) \mathrm{d} \boldsymbol{\psi}=\int p\left(\theta_{i} \mid \boldsymbol{\psi}, \boldsymbol{y}\right) p(\boldsymbol{\psi} \mid \boldsymbol{y}) \mathrm{d} \boldsymbol{\psi}\]</span></p>
<p>and for each element of the hyperparameter vector</p>
<p><span class="math display">\[p\left(\psi_{k} \mid \boldsymbol{y}\right)=\int p(\boldsymbol{\psi} \mid \boldsymbol{y}) \mathrm{d} \boldsymbol{\psi}_{-k}\]</span></p>
<p>Thus, we need to perform the following tasks:
(i) compute <span class="math inline">\(p(\boldsymbol{\psi} \mid \boldsymbol{y})\)</span>, from which also all the relevant marginals p(ùúìk|y) can be
obtained;
(ii) compute <span class="math inline">\(p\left(\theta_{i} \mid \boldsymbol{\psi}, \boldsymbol{y}\right)\)</span> which is needed to compute the parameter marginal posteriors <span class="math inline">\(p\left(\theta_{i} \mid \boldsymbol{y}\right)\)</span></p>
<p>The INLA approach exploits the assumptions of the model to produce a numerical
approximation to the posteriors of interest based on the Laplace approximation
method introduced in Section 4.7 (Tierney and Kadane, 1986).
The first task (i) consists of the computation of an approximation to the joint
posterior of the hyperparameters as:</p>
<p><span class="math display">\[\begin{aligned} p(\boldsymbol{\psi} \mid \boldsymbol{y}) &amp;=\frac{p(\boldsymbol{\theta}, \boldsymbol{\psi} \mid \boldsymbol{y})}{p(\boldsymbol{\theta} \mid \boldsymbol{\psi}, \boldsymbol{y})} \\ &amp;=\frac{p(\boldsymbol{y} \mid \boldsymbol{\theta}, \boldsymbol{\psi}) p(\boldsymbol{\theta}, \boldsymbol{\psi})}{p(\boldsymbol{y})} \frac{1}{p(\boldsymbol{\theta} \mid \boldsymbol{\psi}, \boldsymbol{y})} \\ &amp;=\frac{p(\boldsymbol{y} \mid \boldsymbol{\theta}, \boldsymbol{\psi}) p(\boldsymbol{\theta} \mid \boldsymbol{\psi}) p(\boldsymbol{\psi})}{p(\boldsymbol{y})} \frac{1}{p(\boldsymbol{\theta} \mid \boldsymbol{\psi}, \boldsymbol{y})} \\ &amp; \propto \frac{p(\boldsymbol{y} \mid \boldsymbol{\theta}, \boldsymbol{\psi}) p(\boldsymbol{\theta} \mid \boldsymbol{\psi}) p(\boldsymbol{\psi})}{p(\boldsymbol{\theta} \mid \boldsymbol{\psi}, \boldsymbol{y})} \\ &amp;\left.\approx \frac{p(\boldsymbol{y} \mid \boldsymbol{\theta}, \boldsymbol{\psi}) p(\boldsymbol{\theta} \mid \boldsymbol{\psi}) p(\boldsymbol{\psi})}{\tilde{p}(\boldsymbol{\theta} \mid \boldsymbol{\psi}, \boldsymbol{y})}\right|_{\boldsymbol{\theta}=\boldsymbol{\theta}^{*} \boldsymbol{\psi}}=: \tilde{p}(\boldsymbol{\psi} \mid \boldsymbol{y}) \end{aligned}\]</span></p>
<p>where <span class="math inline">\(\tilde{p}(\theta \mid \psi, y)\)</span> is the Gaussian approximation ‚Äì given by the Laplace method ‚Äì of <span class="math inline">\(p(\theta \mid \psi, y)\)</span> and <span class="math inline">\(\theta^{*}(\psi)\)</span> is the mode for a given <span class="math inline">\(\psi\)</span> the Gaussian approximation turns out to be accurate since <span class="math inline">\(p(\theta \mid \psi, y)\)</span>appears to be almost Gaussian as it is a priori dis-
tributed like a GMRF, y is generally not informative and the observation distribution
is usually well-behaved.
The second task (ii) is slightly more complex, because in general there will
be more elements in ùúΩ than in ùùç, and thus this computation is more expensive.
A first easy possibility is to approximate the posterior conditional distributions
<span class="math inline">\(p(\theta \mid \psi, y)\)</span> directly as the marginals from <span class="math inline">\(\tilde{p}(\theta \mid \psi, y)\)</span> i.e.¬†using a Normal distribution, where the Cholesky decomposition is used for the precision matrix (Rue
and Martino, 2007). While this is very fast, the approximation is generally
not very good. The second possibility is to rewrite the vector of parameters as
<span class="math inline">\(\theta=\left(\theta_{i}, \theta_{-i}\right)\)</span> and use again Laplace approximation to obtain</p>
<p><span class="math display">\[\begin{aligned} p\left(\theta_{i} \mid \boldsymbol{\psi}, \boldsymbol{y}\right) &amp;=\frac{p\left(\left(\theta_{i}, \boldsymbol{\theta}_{-i}\right) \mid \boldsymbol{\psi}, \boldsymbol{y}\right)}{p\left(\boldsymbol{\theta}_{-i} \mid \theta_{i}, \boldsymbol{\psi}, \boldsymbol{y}\right)} \\ &amp;=\frac{p(\boldsymbol{\theta}, \boldsymbol{\psi} \mid \boldsymbol{y})}{p(\boldsymbol{\psi} \mid \boldsymbol{y})} \frac{1}{p\left(\boldsymbol{\theta}_{-i} \mid \theta_{i}, \boldsymbol{\psi}, \boldsymbol{y}\right)} \\ &amp; \propto \frac{p(\boldsymbol{\theta}, \boldsymbol{\psi} \mid \boldsymbol{y})}{p\left(\boldsymbol{\theta}_{-i} \mid \theta_{i}, \boldsymbol{\psi}, \boldsymbol{y}\right)} \\ &amp;\left.\approx \frac{p(\boldsymbol{\theta}, \boldsymbol{\psi} \mid \boldsymbol{y})}{\tilde{p}\left(\boldsymbol{\theta}_{-i} \mid \theta_{i}, \boldsymbol{\psi}, \boldsymbol{y}\right)}\right|_{\boldsymbol{\theta}_{-i}=\boldsymbol{\theta}_{-i}^{*}\left(\theta_{i}, \boldsymbol{\psi}\right)}=: \tilde{p}\left(\theta_{i} \mid \boldsymbol{\psi}, \boldsymbol{y}\right) \end{aligned}\]</span></p>
<p>where <span class="math inline">\(\tilde{p}\left(\boldsymbol{\theta}_{-i} \mid \theta_{i}, \boldsymbol{\psi}, \boldsymbol{y}\right)\)</span> is the Laplace Gaussian approximation to <span class="math inline">\(p\left(\boldsymbol{\theta}_{-i} \mid \theta_{i}, \boldsymbol{\psi}, \boldsymbol{y}\right)\)</span> and <span class="math inline">\(\boldsymbol{\theta}_{-i}^{*}\left(\theta_{i}, \boldsymbol{\psi}\right)\)</span> is its mode. Because the random variables <span class="math inline">\(\theta_{-i} \mid \theta_{i}, \psi, y\)</span> re in general
reasonably Normal, the approximation provided by (4.20) typically works very
well. This strategy, however, can be very expensive in computational terms as <span class="math inline">\(\tilde{p}\left(\theta_{-i} \mid \theta_{i}, \psi, y\right)\)</span> must be recomputed for each value of ùúΩ and ùùç (some modifications
to the Laplace approximation in order to reduce the computational costs are
described in Rue et al., 2009).</p>
<p>Operationally, INLA proceeds as follows:
(i) first it explores the hyperparameter joint posterior distribution <span class="math inline">\(\tilde{p}(\psi \mid \boldsymbol{y})\)</span> of Eq. (4.18) in a nonparametric way, in order to detect good points <span class="math inline">\(\left\{\psi^{(j)}\right\}\)</span> for
the numerical integration required in Eq. (4.22). Rue et al.¬†(2009) propose
two different exploration schemes, both requiring a reparameterization of
the <span class="math inline">\(\psi\)</span>-space ‚Äì in order to deal with more regular densities ‚Äì through the
following steps:</p>
<ol style="list-style-type: lower-alpha">
<li>Locate the mode <span class="math inline">\(\psi^{*}\)</span> of <span class="math inline">\(\tilde{p}(\boldsymbol{\psi} \mid \boldsymbol{y})\)</span> by optimizing log <span class="math inline">\(\tilde{p}(\boldsymbol{\psi} \mid \boldsymbol{y})\)</span> with respect to
ùùç (e.g., through the Newton‚ÄìRaphson method).</li>
<li>Compute the negative Hessian <span class="math inline">\(H\)</span> at the modal configuration.</li>
<li>Compute the eigen-decomposition <span class="math inline">\(\mathbf{\Sigma}=\boldsymbol{V} \Lambda^{1 / 2} \boldsymbol{V}^{\prime}\)</span>, with <span class="math inline">\(\Sigma=H^{-1}\)</span>.</li>
<li>Define the new variable z, with standardized and mutually orthogonal
components, such that:</li>
</ol>
<p><span class="math display">\[\boldsymbol{\psi}(z)=\boldsymbol{\psi}^{*}+\boldsymbol{V} \Lambda^{1 / 2} z\]</span></p>
<p>The first exploration scheme (named grid strategy) builds, using the
z-parameterization, a grid of points associated with the bulk of the mass of <span class="math inline">\(\tilde{p}(\boldsymbol{\psi} \mid \boldsymbol{y})\)</span> . This approach has a computational cost which grows exponentially
with the number of hyperparameters; therefore the advice is to adopt it
when K, the dimension of <span class="math inline">\(\psi\)</span>, is lower than 4. Otherwise, the second explo-
ration scheme, named central composite design (CCD) strategy, should be
used as it reduces the computational costs. With the CCD approach, the
integration problem is seen as a design problem; using the mode <span class="math inline">\(\psi^{*}\)</span> and the
Hessian H, some relevant points in the ùùç-space are selected for performing
a second-order approximation to a response variable (see Section 6.5 of
Rue et al., 2009 for details). In general, the CCD strategy uses much less
points, but still is able to capture the variability of the hyperparameter
distribution. For this reason it is the default option in R-INLA.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="prd.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="applications.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": true,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin"],
"google": false
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/NiccoloSalvini/Thesis/edit/master/06-bayesian.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Niccolo_Salvini_Thesis.pdf", "Niccolo_Salvini_Thesis.epub"],
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"search": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
