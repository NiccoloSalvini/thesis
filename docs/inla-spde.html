<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 5 INLA computation | End-to-End Real Estate Rental app, a Bayesian spatial modelling approach wtih INLA</title>
  <meta name="description" content="Niccol√≤ Salvini master‚Äôs thesis project" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 5 INLA computation | End-to-End Real Estate Rental app, a Bayesian spatial modelling approach wtih INLA" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://niccolosalvini.github.io/Thesis/" />
  <meta property="og:image" content="https://niccolosalvini.github.io/Thesis/images/spat-touch.png" />
  <meta property="og:description" content="Niccol√≤ Salvini master‚Äôs thesis project" />
  <meta name="github-repo" content="NiccoloSalvini/Thesis" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 INLA computation | End-to-End Real Estate Rental app, a Bayesian spatial modelling approach wtih INLA" />
  
  <meta name="twitter:description" content="Niccol√≤ Salvini master‚Äôs thesis project" />
  <meta name="twitter:image" content="https://niccolosalvini.github.io/Thesis/images/spat-touch.png" />

<meta name="author" content="Niccol√≤ Salvini" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  <link rel="apple-touch-icon-precomposed" sizes="120x120" href="images/spatial.png" />
  <link rel="shortcut icon" href="images/favicon.ico" type="image/x-icon" />
<link rel="prev" href="datapres.html"/>
<link rel="next" href="prd.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-171723874-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-171723874-1');
</script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="css\style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"> End-to-End Real Estate Rental app, a Bayesian spatial modelling approach wtih INLA</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preliminary Content</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#abstract"><i class="fa fa-check"></i>Abstract</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#dedication"><i class="fa fa-check"></i>Dedication</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="scraping.html"><a href="scraping.html"><i class="fa fa-check"></i><b>2</b> Scraping</a><ul>
<li class="chapter" data-level="2.1" data-path="scraping.html"><a href="scraping.html#what-is-web-scraping"><i class="fa fa-check"></i><b>2.1</b> What is Web Scraping</a></li>
<li class="chapter" data-level="2.2" data-path="scraping.html"><a href="scraping.html#scraping-best-practices-and-robot.txt"><i class="fa fa-check"></i><b>2.2</b> Scraping Best Practices and Robot.txt</a></li>
<li class="chapter" data-level="2.3" data-path="scraping.html"><a href="scraping.html#user-agents-proxies-handlers"><i class="fa fa-check"></i><b>2.3</b> User agents, Proxies, Handlers</a><ul>
<li class="chapter" data-level="2.3.1" data-path="scraping.html"><a href="scraping.html#user-agents-spoofing"><i class="fa fa-check"></i><b>2.3.1</b> User agents Spoofing</a></li>
<li class="chapter" data-level="2.3.2" data-path="scraping.html"><a href="scraping.html#handlers"><i class="fa fa-check"></i><b>2.3.2</b> Handlers</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="scraping.html"><a href="scraping.html#scraping-with-rvest"><i class="fa fa-check"></i><b>2.4</b> Scraping with <code>rvest</code></a><ul>
<li class="chapter" data-level="2.4.1" data-path="scraping.html"><a href="scraping.html#parallel-computing"><i class="fa fa-check"></i><b>2.4.1</b> Parallel Computing</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="scraping.html"><a href="scraping.html#further-improvements"><i class="fa fa-check"></i><b>2.5</b> Further Improvements</a></li>
<li class="chapter" data-level="2.6" data-path="scraping.html"><a href="scraping.html#legal-challenges-ancora-non-validato"><i class="fa fa-check"></i><b>2.6</b> Legal Challenges (ancora non validato)</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="Infrastructure.html"><a href="Infrastructure.html"><i class="fa fa-check"></i><b>3</b> Infrastructure</a><ul>
<li class="chapter" data-level="3.1" data-path="Infrastructure.html"><a href="Infrastructure.html#scheduler"><i class="fa fa-check"></i><b>3.1</b> Scheduler</a><ul>
<li class="chapter" data-level="3.1.1" data-path="Infrastructure.html"><a href="Infrastructure.html#cron-jobs"><i class="fa fa-check"></i><b>3.1.1</b> Cron Jobs</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="Infrastructure.html"><a href="Infrastructure.html#docker-container"><i class="fa fa-check"></i><b>3.2</b> Docker Container</a><ul>
<li class="chapter" data-level="3.2.1" data-path="Infrastructure.html"><a href="Infrastructure.html#what-is-docker"><i class="fa fa-check"></i><b>3.2.1</b> What is Docker?</a></li>
<li class="chapter" data-level="3.2.2" data-path="Infrastructure.html"><a href="Infrastructure.html#what-are-the-main-andvantages-of-using-docker"><i class="fa fa-check"></i><b>3.2.2</b> What are the main andvantages of using Docker</a></li>
<li class="chapter" data-level="3.2.3" data-path="Infrastructure.html"><a href="Infrastructure.html#dockerfile"><i class="fa fa-check"></i><b>3.2.3</b> Dockerfile</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="Infrastructure.html"><a href="Infrastructure.html#api"><i class="fa fa-check"></i><b>3.3</b> API</a><ul>
<li class="chapter" data-level="3.3.1" data-path="Infrastructure.html"><a href="Infrastructure.html#what-is-an-api"><i class="fa fa-check"></i><b>3.3.1</b> What is an API</a></li>
<li class="chapter" data-level="3.3.2" data-path="Infrastructure.html"><a href="Infrastructure.html#plumber-api"><i class="fa fa-check"></i><b>3.3.2</b> Plumber API</a></li>
<li class="chapter" data-level="3.3.3" data-path="Infrastructure.html"><a href="Infrastructure.html#immobiliare-api-design"><i class="fa fa-check"></i><b>3.3.3</b> Immobiliare API design</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="Infrastructure.html"><a href="Infrastructure.html#aws-ec2-server"><i class="fa fa-check"></i><b>3.4</b> AWS EC2 server</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="datapres.html"><a href="datapres.html"><i class="fa fa-check"></i><b>4</b> Data Presentation</a><ul>
<li class="chapter" data-level="4.1" data-path="datapres.html"><a href="datapres.html#data-glimpse"><i class="fa fa-check"></i><b>4.1</b> Data Glimpse</a></li>
<li class="chapter" data-level="4.2" data-path="datapres.html"><a href="datapres.html#explorative-analysis"><i class="fa fa-check"></i><b>4.2</b> Explorative Analysis</a><ul>
<li class="chapter" data-level="4.2.1" data-path="datapres.html"><a href="datapres.html#semivariogram-covariogram"><i class="fa fa-check"></i><b>4.2.1</b> Semivariogram Covariogram</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="datapres.html"><a href="datapres.html#gaussian-random-fields"><i class="fa fa-check"></i><b>4.3</b> Gaussian random fields</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="inla-spde.html"><a href="inla-spde.html"><i class="fa fa-check"></i><b>5</b> INLA computation</a><ul>
<li class="chapter" data-level="5.1" data-path="inla-spde.html"><a href="inla-spde.html#inla"><i class="fa fa-check"></i><b>5.1</b> INLA</a></li>
<li class="chapter" data-level="5.2" data-path="inla-spde.html"><a href="inla-spde.html#laplace-approximation"><i class="fa fa-check"></i><b>5.2</b> Laplace Approximation</a></li>
<li class="chapter" data-level="5.3" data-path="inla-spde.html"><a href="inla-spde.html#the-class-of-latent-gaussian-models"><i class="fa fa-check"></i><b>5.3</b> The Class of Latent Gaussian Models</a></li>
<li class="chapter" data-level="5.4" data-path="inla-spde.html"><a href="inla-spde.html#approximate-bayesian-inference-with-inla"><i class="fa fa-check"></i><b>5.4</b> Approximate Bayesian inference with INLA</a></li>
<li class="chapter" data-level="5.5" data-path="inla-spde.html"><a href="inla-spde.html#stochastic-partial-differential-equation"><i class="fa fa-check"></i><b>5.5</b> Stochastic partial differential equation</a></li>
<li class="chapter" data-level="5.6" data-path="inla-spde.html"><a href="inla-spde.html#the-projector-matrix"><i class="fa fa-check"></i><b>5.6</b> The Projector Matrix</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="prd.html"><a href="prd.html"><i class="fa fa-check"></i><b>6</b> Point Referenced Data Modelling</a></li>
<li class="chapter" data-level="7" data-path="applications.html"><a href="applications.html"><i class="fa fa-check"></i><b>7</b> Applications</a><ul>
<li class="chapter" data-level="7.1" data-path="applications.html"><a href="applications.html#example-one"><i class="fa fa-check"></i><b>7.1</b> Example one</a></li>
<li class="chapter" data-level="7.2" data-path="applications.html"><a href="applications.html#example-two"><i class="fa fa-check"></i><b>7.2</b> Example two</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="final-words.html"><a href="final-words.html"><i class="fa fa-check"></i><b>8</b> Final Words</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/NiccoloSalvini/tesi-prova" target="blank"> See Github Repository</a></li>
<li><a href="https://niccolosalvini.netlify.app/">About The Author</a></li>
<li><a Proudly published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">End-to-End Real Estate Rental app, a Bayesian spatial modelling approach wtih INLA</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="inla-spde" class="section level1">
<h1><span class="header-section-number">Chapter 5</span> INLA computation</h1>
<p><strong>va parafrasatoo</strong></p>
<p>Several types of models are used with spatial and spatio-temporal data, depend-
ing on the aim of the study. If we are interested in summarizing spatial and
spatio-temporal variation between areas using risks or probabilities then we
could use statistical methods like disease mapping to compare maps and identify
clusters. Moran Index is extensively used to check for spatial autocorrelation
(Moran, 1950), while the scan statistics, implemented in SaTScan (Killdorf, 1997),
has been used for cluster detection and to perform geographical surveillance in
a non-Bayesian approach. The same types of models can also be used in studies
where there is an aetiological aim to assess the potential effect of risk factors on
outcomes.
A different type of study considers the quantification of the risk of experienc-
ing an outcome as the distance from a certain source increases. This is typically
framed in an environmental context, so that the source could be a point (e.g., waste
site, radio transmitter) or a line (e.g., power line, road). In this case, the meth-
ods typically used vary from nonparametric tests proposed by Stone (1988) to the
parametric approach introduced by Diggle et al.¬†(1998).
In a different context, when the interest lies in mapping continuous spatial (or
spatio-temporal) variables, which are measured only at a finite set of specific points
in a given region, and in predicting their values at unobserved locations, geostatis-
tical methods ‚Äì such as kriging ‚Äì are employed (Cressie, 1991; Stein, 1991). This
may play a significant role in environmental risk assessment in order to identify
areas where the risk of exceeding potentially harmful thresholds is higher.
Bayesian methods to deal with spatial and spatio-temporal data started to appear
around year 2000, with the development of Markov chain Monte Carlo (MCMC)
simulative methods (Casella and George, 1992; Gilks et al., 1996). Before that the
Bayesian approach was almost only used for theoretical models and found little
applications in real case studies due to the lack of numerical/analytical or simula-
tive tools to compute posterior distributions. The advent of MCMC has triggered
the possibility for researchers to develop complex models on large datasets without the need of imposing simplified structures. Probably the main contribution to spatial
and spatio-temporal statistics is the one of Besag et al.¬†(1991), who developed
the Besag‚ÄìYork‚ÄìMolli√© (BYM) method (see Chapter 6) which is commonly used
for disease mapping, while Banerjee et al.¬†(2004), Diggle and Ribeiro (2007) and
Cressie and Wikle (2011) have concentrated on Bayesian geostatistical models.
The main advantage of the Bayesian approach resides in its taking into account
uncertainty in the estimates/predictions, and its flexibility and capability of dealing
with issues like missing data. In the book, we follow this paradigm and introduce
the Bayesian philosophy and inference in Chapter 3, while in Chapter 4 we review
Bayesian computation tools, but the reader could also find interesting the follow-
ing: Knorr-Held (2000) and Best et al.¬†(2005) for disease mapping and Diggle et al.
(1998) for a modeling approach for continuous spatial data and for prediction</p>
<p><strong>va parafrasatoo</strong></p>
<div id="inla" class="section level2">
<h2><span class="header-section-number">5.1</span> INLA</h2>
<p>For many years, Bayesian inference has relied upon Markov chain Monte Carlo methods (Gilks et al.¬†1996; Brooks et al.¬†2011) to compute the joint posterior distribution of the model parameters. This is usually computationally very expensive as this distribution is often in a space of high dimension.</p>
<p>Havard Rue, Martino, and Chopin (2009) propose a novel approach that makes Bayesian inference faster. First of all, rather than aiming at estimating the joint posterior distribution of the model parameters, they suggest focusing on individual posterior marginals of the model parameters. In many cases, marginal inference is enough to make inference of the model parameters and latent effects, and there is no need to deal with multivariate posterior distributions that are difficult to obtain. Secondly, they focus on models that can be expressed as latent Gaussian Markov random fields (GMRF). This provides the computational advantages (see Rue and Held 2005) that reduce computation time of model fitting. Furthermore, Havard Rue, Martino, and Chopin (2009) develop a new approximation to the posterior marginal distributions of the model parameters based on the Laplace approximation (see, for example, MacKay 2003). A recent review on INLA can be found in Rue et al.¬†(2017).</p>
</div>
<div id="laplace-approximation" class="section level2">
<h2><span class="header-section-number">5.2</span> Laplace Approximation</h2>
<p>An alternative approach to the simulation-based MC integration is analytic approx-
imation with the Laplace method. Suppose we are interested in computing the
following integral:</p>
<p><span class="math display">\[\int f(x) \mathrm{d} x=\int \exp (\log f(x)) \mathrm{d} x\]</span></p>
<p>where <span class="math inline">\(f(x)\)</span> is the density function of a random variable X. We represent <span class="math inline">\(log f(x)\)</span> by
means of a Taylor series expansion evaluated in x = x0:</p>
<p><span class="math display">\[\log f(x) \approx \log f\left(x_{0}\right)+\left.\left(x-x_{0}\right) \frac{\partial \log f(x)}{\partial x}\right|_{x=x_{0}}+\left.\frac{\left(x-x_{0}\right)^{2}}{2} \frac{\partial^{2} \log f(x)}{\partial x^{2}}\right|_{x=x_{0}}\]</span></p>
<p>If x0 is set equal to the mode <span class="math inline">\(x‚àó = argmax\)</span>, log f(x) then log f(x)<span class="math inline">\(\left.\frac{\partial \log f(x)}{\partial x}\right|_{x=x^{*}}=0\)</span> and the approximation becomes</p>
<p><span class="math display">\[\log f(x) \approx \log f\left(x^{*}\right)+\left.\frac{\left(x-x^{*}\right)^{2}}{2} \frac{\partial^{2} \log f(x)}{\partial x^{2}}\right|_{x=x^{*}}\]</span></p>
<p>The integral of interest is then approximated as follows:</p>
<p><span class="math display">\[\int f(x) \mathrm{d} x \approx \int \exp \left(\log f\left(x^{*}\right)+\left.\frac{\left(x-x^{*}\right)^{2}}{2} \frac{\partial^{2} \log f(x)}{\partial x^{2}}\right|_{x=x^{*}}\right) \mathrm{d} x\]</span></p>
<p><span class="math display">\[=\exp \left(\log f\left(x^{*}\right)\right) \int \exp \left(\left.\frac{\left(x-x^{*}\right)^{2}}{2} \frac{\partial^{2} \log f(x)}{\partial x^{2}}\right|_{x=x^{*}}\right) \mathrm{d} x\]</span></p>
<p>where the integrand can be associated with the density of a Normal distribution. In
fact, by setting <span class="math display">\[\sigma^{2 *}=-1 /\left.\frac{\partial^{2} \log f(x)}{\partial x^{2}}\right|_{x=x^{*}}\]</span> we obtain:</p>
<p><span class="math display">\[\int f(x) \mathrm{d} x \approx \exp \left(\log f\left(x^{*}\right)\right) \int \exp \left(-\frac{\left(x-x^{*}\right)^{2}}{2 \sigma^{2 *}}\right) \mathrm{d} x\]</span></p>
<p>where the integrand is the kernel of a Normal distribution with mean equal to x‚àó
and variance <span class="math inline">\(\sigma^{2*}\)</span>. More precisely, the integral evaluated in the interval <span class="math inline">\((\alpha, \beta)\)</span> is
approximated by:</p>
<p><span class="math display">\[\int_{\alpha}^{\beta} f(x) \mathrm{d} x \approx f\left(x^{*}\right) \sqrt{2 \pi \sigma^{2 *}}(\Phi(\beta)-\Phi(\alpha))\]</span></p>
<p>where <span class="math inline">\(\Phi(‚ãÖ)\)</span> denotes the cumulative density function of th <span class="math inline">\(Normal(x_i, \sigma^{2*})\)</span> distri-
bution.</p>
<p><strong>qui volendo un esempio fatto da me</strong></p>
</div>
<div id="the-class-of-latent-gaussian-models" class="section level2">
<h2><span class="header-section-number">5.3</span> The Class of Latent Gaussian Models</h2>
<p>The first step in defining a latent Gaussian model within the Bayesian framework
is to identify a distribution for the observed data y = (y1, ‚Ä¶ , yn). A very general
approach consists in specifying a distribution for yi characterized by a parameter
ùúôi (usually the mean E(yi)) defined as a function of a structured additive predictor
ùúÇi through a link function g(‚ãÖ), such that g(ùúôi) = ùúÇi. The additive linear predictor ùúÇi
is defined as follows:</p>
<p><span class="math display">\[\eta_{i}=\beta_{0}+\sum_{m=1}^{M} \beta_{m} x_{m i}+\sum_{l=1}^{L} f_{l}\left(z_{l i}\right)\]</span></p>
<p>Here ùõΩ0 is a scalar representing the intercept; the coefficients <span class="math inline">\(\boldsymbol{\beta}=\left\{\beta_{1}, \ldots, \beta_{M}\right\}\)</span>
quantify the (linear) effect of some covariates <span class="math inline">\(\boldsymbol{x}=\left(\boldsymbol{x}_{1}, \ldots, \boldsymbol{x}_{M}\right)\)</span> on the response; and
<span class="math inline">\(f=\left\{f_{1}(\cdot), \ldots, f_{L}(\cdot)\right\}\)</span> is a collection of functions defined in terms of a set of covariates <span class="math inline">\(z = (z_1, ‚Ä¶ , z_L)\)</span>. The terms <span class="math inline">\(f_l(‚ãÖ)\)</span> can assume different forms such as smooth and</p>
<p>nonlinear effects of covariates, time trends and seasonal effects, random intercept
and slopes as well as temporal or spatial random effects. For this reason, the class of
latent Gaussian models is very flexible and can accomodate a wide range of models
ranging from generalized and dynamic linear models to spatial and spatio-temporal
models (see Martins et al., 2013 for a review).
We collect all the latent (nonobservable) components of interest for the inference
in a set of parameters named ùúΩ defined as <span class="math inline">\(\boldsymbol{\theta}=\left\{\beta_{0}, \boldsymbol{\beta}, \boldsymbol{f}\right\}\)</span>. Moreover, we denote with
<span class="math inline">\(\psi=\left\{\psi_{1}, \ldots, \psi_{K}\right\}\)</span> the vector of the K hyperparameters. By assuming conditional
independence, the distribution of the n observations (all coming from the same
distribution family) is given by the likelihood</p>
<p><span class="math display">\[p(\boldsymbol{y} \mid \boldsymbol{\theta}, \boldsymbol{\psi})=\prod_{i=1}^{n} p\left(y_{i} \mid \theta_{i}, \boldsymbol{\psi}\right)\]</span></p>
<p>where each data point yi is connected to only one element ùúÉi in the latent field <span class="math inline">\(\theta\)</span>.
Martins et al.¬†(2013) discuss the possibility of relaxing this assumption assuming
that each observation may be connected with a linear combination of elements in
<span class="math inline">\(\theta\)</span>; moreover, they take into account the case when the data belong to several distri-
butions, i.e., the multiple likelihoods case.</p>
<p>We assume a multivariate Normal prior on ùúΩ with mean ùüé and precision matrix
<span class="math inline">\(Q(\psi)\)</span>, i.e., <span class="math inline">\(\boldsymbol{\theta} \sim \operatorname{Normal}\left(\mathbf{0}, \boldsymbol{Q}^{-1}(\boldsymbol{\psi})\right)\)</span> with density function given by</p>
<p><span class="math display">\[p(\theta \mid \psi)=(2 \pi)^{-n / 2}|Q(\psi)|^{1 / 2} \exp \left(-\frac{1}{2} \theta^{\prime} Q(\psi) \theta\right)\]</span></p>
<p>where | ‚ãÖ | denotes the matrix determinant and ‚Ä≤ is used for the transpose operation.
The components of the latent Gaussian field ùúΩ are supposed to be conditionally
independent with the consequence that <span class="math inline">\(Q(\psi)\)</span> is a sparse precision matrix.8 This
specification is known as Gaussian Markov random field (GMRF, Rue and Held,
2005). Note that the sparsity of the precision matrix gives rise to computational ben-
efits when making inference with GMRFs. In fact, linear algebra operations can be
performed using numerical methods for sparse matrices, resulting in a considerable
computational gain (see Rue and Held, 2005 for algorithms).
The joint posterior distribution of ùúΩ and <span class="math inline">\(\psi\)</span> is given by the product of the likelihood
(4.13), of the GMRF density (4.14) and of the hyperparameter prior distribution
<span class="math inline">\(p(\psi)\)</span>:</p>
<p><span class="math display">\[
\begin{aligned} p(\boldsymbol{\theta}, \boldsymbol{\psi} \mid \boldsymbol{y}) &amp; \propto p(\boldsymbol{\psi}) \times p(\boldsymbol{\theta} \mid \boldsymbol{\psi}) \times p(\boldsymbol{y} \mid \boldsymbol{\theta}, \boldsymbol{\psi}) \\ &amp; \propto p(\boldsymbol{\psi}) \times p(\boldsymbol{\theta} \mid \boldsymbol{\psi}) \times \prod_{i=1}^{n} p\left(y_{i} \mid \theta_{i}, \boldsymbol{\psi}\right) \\ &amp; \propto p(\boldsymbol{\psi}) \times|\boldsymbol{Q}(\boldsymbol{\psi})|^{1 / 2} \exp \left(-\frac{1}{2} \boldsymbol{\theta}^{\prime} \boldsymbol{Q}(\boldsymbol{\psi}) \boldsymbol{\theta}\right) \times \prod_{i=1}^{n} \exp \left(\log \left(p\left(y_{i} \mid \theta_{i}, \boldsymbol{\psi}\right)\right)\right) \\ &amp; \propto p(\boldsymbol{\psi}) \times|\boldsymbol{Q}(\boldsymbol{\psi})|^{1 / 2} \exp \left(-\frac{1}{2} \boldsymbol{\theta}^{\prime} \boldsymbol{Q}(\boldsymbol{\psi}) \boldsymbol{\theta}+\sum_{i=1}^{n} \log \left(p\left(y_{i} \mid \theta_{i}, \boldsymbol{\psi}\right)\right)\right) \end{aligned}
\]</span></p>
</div>
<div id="approximate-bayesian-inference-with-inla" class="section level2">
<h2><span class="header-section-number">5.4</span> Approximate Bayesian inference with INLA</h2>
<p>The objectives of Bayesian inference are the marginal posterior distributions for
each element of the parameter vector</p>
<p><span class="math display">\[p\left(\theta_{i} \mid \boldsymbol{y}\right)=\int p\left(\theta_{i}, \boldsymbol{\psi} \mid \boldsymbol{y}\right) \mathrm{d} \boldsymbol{\psi}=\int p\left(\theta_{i} \mid \boldsymbol{\psi}, \boldsymbol{y}\right) p(\boldsymbol{\psi} \mid \boldsymbol{y}) \mathrm{d} \boldsymbol{\psi}\]</span></p>
<p>and for each element of the hyperparameter vector</p>
<p><span class="math display">\[p\left(\psi_{k} \mid \boldsymbol{y}\right)=\int p(\boldsymbol{\psi} \mid \boldsymbol{y}) \mathrm{d} \boldsymbol{\psi}_{-k}\]</span></p>
<p>Thus, we need to perform the following tasks:
(i) compute <span class="math inline">\(p(\boldsymbol{\psi} \mid \boldsymbol{y})\)</span>, from which also all the relevant marginals p(ùúìk|y) can be
obtained;
(ii) compute <span class="math inline">\(p\left(\theta_{i} \mid \boldsymbol{\psi}, \boldsymbol{y}\right)\)</span> which is needed to compute the parameter marginal posteriors <span class="math inline">\(p\left(\theta_{i} \mid \boldsymbol{y}\right)\)</span></p>
<p>The INLA approach exploits the assumptions of the model to produce a numerical
approximation to the posteriors of interest based on the Laplace approximation
method introduced in Section 4.7 (Tierney and Kadane, 1986).
The first task (i) consists of the computation of an approximation to the joint
posterior of the hyperparameters as:</p>
<p><span class="math display">\[\begin{aligned} p(\boldsymbol{\psi} \mid \boldsymbol{y}) &amp;=\frac{p(\boldsymbol{\theta}, \boldsymbol{\psi} \mid \boldsymbol{y})}{p(\boldsymbol{\theta} \mid \boldsymbol{\psi}, \boldsymbol{y})} \\ &amp;=\frac{p(\boldsymbol{y} \mid \boldsymbol{\theta}, \boldsymbol{\psi}) p(\boldsymbol{\theta}, \boldsymbol{\psi})}{p(\boldsymbol{y})} \frac{1}{p(\boldsymbol{\theta} \mid \boldsymbol{\psi}, \boldsymbol{y})} \\ &amp;=\frac{p(\boldsymbol{y} \mid \boldsymbol{\theta}, \boldsymbol{\psi}) p(\boldsymbol{\theta} \mid \boldsymbol{\psi}) p(\boldsymbol{\psi})}{p(\boldsymbol{y})} \frac{1}{p(\boldsymbol{\theta} \mid \boldsymbol{\psi}, \boldsymbol{y})} \\ &amp; \propto \frac{p(\boldsymbol{y} \mid \boldsymbol{\theta}, \boldsymbol{\psi}) p(\boldsymbol{\theta} \mid \boldsymbol{\psi}) p(\boldsymbol{\psi})}{p(\boldsymbol{\theta} \mid \boldsymbol{\psi}, \boldsymbol{y})} \\ &amp;\left.\approx \frac{p(\boldsymbol{y} \mid \boldsymbol{\theta}, \boldsymbol{\psi}) p(\boldsymbol{\theta} \mid \boldsymbol{\psi}) p(\boldsymbol{\psi})}{\tilde{p}(\boldsymbol{\theta} \mid \boldsymbol{\psi}, \boldsymbol{y})}\right|_{\boldsymbol{\theta}=\boldsymbol{\theta}^{*} \boldsymbol{\psi}}=: \tilde{p}(\boldsymbol{\psi} \mid \boldsymbol{y}) \end{aligned}\]</span></p>
<p>where <span class="math inline">\(\tilde{p}(\theta \mid \psi, y)\)</span> is the Gaussian approximation ‚Äì given by the Laplace method ‚Äì of <span class="math inline">\(p(\theta \mid \psi, y)\)</span> and <span class="math inline">\(\theta^{*}(\psi)\)</span> is the mode for a given <span class="math inline">\(\psi\)</span> the Gaussian approximation turns out to be accurate since <span class="math inline">\(p(\theta \mid \psi, y)\)</span>appears to be almost Gaussian as it is a priori dis-
tributed like a GMRF, y is generally not informative and the observation distribution
is usually well-behaved.
The second task (ii) is slightly more complex, because in general there will
be more elements in ùúΩ than in ùùç, and thus this computation is more expensive.
A first easy possibility is to approximate the posterior conditional distributions
<span class="math inline">\(p(\theta \mid \psi, y)\)</span> directly as the marginals from <span class="math inline">\(\tilde{p}(\theta \mid \psi, y)\)</span> i.e.¬†using a Normal distribution, where the Cholesky decomposition is used for the precision matrix (Rue
and Martino, 2007). While this is very fast, the approximation is generally
not very good. The second possibility is to rewrite the vector of parameters as
<span class="math inline">\(\theta=\left(\theta_{i}, \theta_{-i}\right)\)</span> and use again Laplace approximation to obtain</p>
<p><span class="math display">\[\begin{aligned} p\left(\theta_{i} \mid \boldsymbol{\psi}, \boldsymbol{y}\right) &amp;=\frac{p\left(\left(\theta_{i}, \boldsymbol{\theta}_{-i}\right) \mid \boldsymbol{\psi}, \boldsymbol{y}\right)}{p\left(\boldsymbol{\theta}_{-i} \mid \theta_{i}, \boldsymbol{\psi}, \boldsymbol{y}\right)} \\ &amp;=\frac{p(\boldsymbol{\theta}, \boldsymbol{\psi} \mid \boldsymbol{y})}{p(\boldsymbol{\psi} \mid \boldsymbol{y})} \frac{1}{p\left(\boldsymbol{\theta}_{-i} \mid \theta_{i}, \boldsymbol{\psi}, \boldsymbol{y}\right)} \\ &amp; \propto \frac{p(\boldsymbol{\theta}, \boldsymbol{\psi} \mid \boldsymbol{y})}{p\left(\boldsymbol{\theta}_{-i} \mid \theta_{i}, \boldsymbol{\psi}, \boldsymbol{y}\right)} \\ &amp;\left.\approx \frac{p(\boldsymbol{\theta}, \boldsymbol{\psi} \mid \boldsymbol{y})}{\tilde{p}\left(\boldsymbol{\theta}_{-i} \mid \theta_{i}, \boldsymbol{\psi}, \boldsymbol{y}\right)}\right|_{\boldsymbol{\theta}_{-i}=\boldsymbol{\theta}_{-i}^{*}\left(\theta_{i}, \boldsymbol{\psi}\right)}=: \tilde{p}\left(\theta_{i} \mid \boldsymbol{\psi}, \boldsymbol{y}\right) \end{aligned}\]</span></p>
<p>where <span class="math inline">\(\tilde{p}\left(\boldsymbol{\theta}_{-i} \mid \theta_{i}, \boldsymbol{\psi}, \boldsymbol{y}\right)\)</span> is the Laplace Gaussian approximation to <span class="math inline">\(p\left(\boldsymbol{\theta}_{-i} \mid \theta_{i}, \boldsymbol{\psi}, \boldsymbol{y}\right)\)</span> and <span class="math inline">\(\boldsymbol{\theta}_{-i}^{*}\left(\theta_{i}, \boldsymbol{\psi}\right)\)</span> is its mode. Because the random variables <span class="math inline">\(\theta_{-i} \mid \theta_{i}, \psi, y\)</span> re in general
reasonably Normal, the approximation provided by (4.20) typically works very
well. This strategy, however, can be very expensive in computational terms as <span class="math inline">\(\tilde{p}\left(\theta_{-i} \mid \theta_{i}, \psi, y\right)\)</span> must be recomputed for each value of ùúΩ and ùùç (some modifications
to the Laplace approximation in order to reduce the computational costs are
described in Rue et al., 2009).</p>
<p>Operationally, INLA proceeds as follows:
(i) first it explores the hyperparameter joint posterior distribution <span class="math inline">\(\tilde{p}(\psi \mid \boldsymbol{y})\)</span> of Eq. (4.18) in a nonparametric way, in order to detect good points <span class="math inline">\(\left\{\psi^{(j)}\right\}\)</span> for
the numerical integration required in Eq. (4.22). Rue et al.¬†(2009) propose
two different exploration schemes, both requiring a reparameterization of
the <span class="math inline">\(\psi\)</span>-space ‚Äì in order to deal with more regular densities ‚Äì through the
following steps:</p>
<ol style="list-style-type: lower-alpha">
<li>Locate the mode <span class="math inline">\(\psi^{*}\)</span> of <span class="math inline">\(\tilde{p}(\boldsymbol{\psi} \mid \boldsymbol{y})\)</span> by optimizing log <span class="math inline">\(\tilde{p}(\boldsymbol{\psi} \mid \boldsymbol{y})\)</span> with respect to
ùùç (e.g., through the Newton‚ÄìRaphson method).</li>
<li>Compute the negative Hessian <span class="math inline">\(H\)</span> at the modal configuration.</li>
<li>Compute the eigen-decomposition <span class="math inline">\(\mathbf{\Sigma}=\boldsymbol{V} \Lambda^{1 / 2} \boldsymbol{V}^{\prime}\)</span>, with <span class="math inline">\(\Sigma=H^{-1}\)</span>.</li>
<li>Define the new variable z, with standardized and mutually orthogonal
components, such that:</li>
</ol>
<p><span class="math display">\[\boldsymbol{\psi}(z)=\boldsymbol{\psi}^{*}+\boldsymbol{V} \Lambda^{1 / 2} z\]</span></p>
<p>The first exploration scheme (named grid strategy) builds, using the
z-parameterization, a grid of points associated with the bulk of the mass of <span class="math inline">\(\tilde{p}(\boldsymbol{\psi} \mid \boldsymbol{y})\)</span> . This approach has a computational cost which grows exponentially
with the number of hyperparameters; therefore the advice is to adopt it
when K, the dimension of <span class="math inline">\(\psi\)</span>, is lower than 4. Otherwise, the second explo-
ration scheme, named central composite design (CCD) strategy, should be
used as it reduces the computational costs. With the CCD approach, the
integration problem is seen as a design problem; using the mode <span class="math inline">\(\psi^{*}\)</span> and the
Hessian H, some relevant points in the ùùç-space are selected for performing
a second-order approximation to a response variable (see Section 6.5 of
Rue et al., 2009 for details). In general, the CCD strategy uses much less
points, but still is able to capture the variability of the hyperparameter
distribution. For this reason it is the default option in R-INLA.</p>
</div>
<div id="stochastic-partial-differential-equation" class="section level2">
<h2><span class="header-section-number">5.5</span> Stochastic partial differential equation</h2>
<p>approach</p>
<p>As described in the previous chapters, in R-INLA the linear predictor <span class="math inline">\(\eta_{i}\)</span> is defined
using the formula which includes <code>f()</code> terms for nonlinear effects of covariates or
random effects. In the same way, the Mat√©rn GF will be included in the formula
using a proper specification for <code>f()</code>. For making the connection between the linear
predictor<span class="math inline">\(\eta\)</span> i and the formula more explicit, it is convenient to follow Lindgren
and Rue (2015) and to rewrite the linear predictor as:</p>
<p><span class="math inline">\(\eta_{i}=\sum_{k} h_{k}\left(z_{i}^{k}\right)\)</span></p>
<p>where <em>k</em> denotes the <em>k</em> th term of the <code>formula</code>, <span class="math inline">\(z_{i}^{k}\)</span> represents the covariate value for a fixed/nonlinear effect or, in the case of a random effect, the index of a second-order
unit (e.g., area or point ID). The mapping function <span class="math inline">\(h_{k}(\cdot)\)</span> links <span class="math inline">\(z_{i}^{k}\)</span> to the actual value
of the latent field for the <em>k</em> th formula component.
As an example consider the case where the linear predictor <span class="math inline">\(\eta_{i}\)</span> includes a fixed
effect of a covariate (named <code>z1</code> in R), a nonlinear effect (e.g., RW2) of the variable
<code>time</code> (given by a sequence of time points), and a random effect with iid compo-
nents indexed by the variable <code>index.random</code>. Thus the corresponding <code>R-INLA</code>
<code>formula</code> is</p>
<pre><code>
formula &lt;- -1 + z1 + f(time, model=&quot;rw2&quot;) + f(index.random, model=&quot;iid&quot;)
</code></pre>
<p>The default intercept is not included as we specify <code>-1</code> in the formula. With the
new linear pwredictor, we have that <span class="math inline">\(h_{1}\left(z_{i}^{1}\right)\)</span> is equal to <span class="math inline">\(z_{i}^{1} \beta\)</span>,
<span class="math inline">\(h_{2}\left(z_{i}^{2}\right)\)</span> is the smooth effect evaluated at <span class="math inline">\(z_{i}^{2}\)</span> (the ith element of vector time), and <span class="math inline">\(h_{3}\left(z_{i}^{3}\right)\)</span> is the random effect component with index equal to <span class="math inline">\(z_{i}^{3}\)</span> (the ith element of <code>index.random</code>). As usual, the latent field <span class="math inline">\(\theta\)</span> is the joint vector of all the latent Gaussian variables included in the linear predictor.</p>
<p>The formulation only allows each observation to directly depend on
a single element <span class="math inline">\(z_{i}^{k}\)</span> from each effect <span class="math inline">\(h_{k}(\cdot)\)</span> and this does not cover the case when a random effect is defined as a linear combination of temporal or areal values, such
as the SPDE representation. In such cases each observation <span class="math inline">\(y_{i}\)</span> depends on a linear combination of the elements of ùúΩ and the observation distribution will be defined as:</p>
<p><span class="math inline">\(y_{i} \mid \theta, \psi \sim p\left(y_{i} \mid \sum_{j} A_{i j} \theta_{j}, \psi\right)\)</span></p>
<p>instead of <span class="math inline">\(y_{i} \mid \boldsymbol{\theta}_{i}, \boldsymbol{\psi} \sim p\left(y_{i} \mid \theta_{i}, \boldsymbol{\psi}\right)\)</span> as in Eq. (4.13). The term <span class="math inline">\(A_{ij}\)</span> is the generic element
of the matrix <strong>A</strong> referred to as the observation or projector matrix. In Sections 6.7.2 and 6.7.3, we will show how to create the <strong>A</strong> matrix using the helper func
tion <code>inla.spde.matrix.A</code> and how to include it in the inla call through the <code>control.predictor</code> option. The <strong>A</strong> matrix defines a mapping between the
spatial latent field (defined on the mesh) and the observations (defined in a set
of locations) and this allows the SPDE models to be treated as standard indexed
random effects (the mapping is done by placing appropriate <span class="math inline">\(\varphi_{g}(s)\)</span> values in the
<strong>A</strong> matrix, see Eq. (6.18)). Internally, <code>R-INLA</code> creates a new linear predictor <span class="math inline">\(\eta^{\star}\)</span>
defined as a linear combination of the original one <span class="math inline">\(\eta\)</span>:</p>
<p><span class="math inline">\(\eta^{\star}=A \eta\)</span></p>
<p>and in this case the likelihood is linked to the latent field through<span class="math inline">\(\eta^{\star}_{i}\)</span> instead of
<span class="math inline">\(\eta_{i}\)</span>:</p>
<p><span class="math inline">\(p(\boldsymbol{y} \mid \boldsymbol{\theta}, \boldsymbol{\psi})=\prod_{i=1}^{n} p\left(y_{i} \mid \eta_{i}^{\star}, \boldsymbol{\psi}\right)\)</span></p>
</div>
<div id="the-projector-matrix" class="section level2">
<h2><span class="header-section-number">5.6</span> The Projector Matrix</h2>
<p>We need to construct a projection matrix <strong>A</strong> to project the GRF from the observations to thetriangulation vertices. The matrix <strong>A</strong> as the number of rows equal to the number of observations, and the number of columns equal to the number of vertices of the triangulation. Row <span class="math inline">\(i\)</span> of <strong>A</strong> corresponding to an observation at location <span class="math inline">\(s_{i}\)</span> ossibly has three non-zero values at the columns that correspond to the vertices of the triangle that contains the location. If <span class="math inline">\(s_{i}\)</span> within the triangle, these values are equal to the barycentric coordinates. That is, they are proportional to the areas of each of the three subtriangles defined by the location <span class="math inline">\(s_{i}\)</span>
and the triangle‚Äôs vertices, and sum to 1. If <span class="math inline">\(s_{i}\)</span> s equal to a vertex of the triangle, row<br />
<span class="math inline">\(i\)</span> has just one non-zero value equal to 1 at the column that corresponds to the vertex. Intuitively, the value <span class="math inline">\(Z(\boldsymbol{s})\)</span> at a location that lies within one triangle is the projection of the plane formed by the triangle vertices weights at location <span class="math inline">\(s\)</span> .</p>
<p>An example of a projection matrix is given below. This projection matrix projects <span class="math inline">\(n\)</span> observations to <span class="math inline">\(G\)</span> triangulation vertices. The first row of the matrix corresponds to an observation with location that coincides with vertex number 3. The second and last rows correspond to observations with locations lying within triangles.</p>
<p><span class="math inline">\(4=\left[\begin{array}{ccccc}A_{11} &amp; A_{12} &amp; A_{13} &amp; \ldots &amp; A_{1 G} \\ A_{21} &amp; A_{22} &amp; A_{23} &amp; \ldots &amp; A_{2 G} \\ \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ A_{n 1} &amp; A_{n 2} &amp; A_{n 3} &amp; \ldots &amp; A_{n G}\end{array}\right]=\left[\begin{array}{ccccc}0 &amp; 0 &amp; 1 &amp; \ldots &amp; 0 \\ A_{21} &amp; A_{22} &amp; 0 &amp; \ldots &amp; A_{2 G} \\ \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ A_{n 1} &amp; A_{n 2} &amp; A_{n 3} &amp; \ldots &amp; 0\end{array}\right]\)</span></p>
<p>igure 8.6 shows a location <span class="math inline">\(s\)</span> that lies within one of the triangles of a triangulated mesh. The value of the process <span class="math inline">\(Z(\cdot)\)</span> at <span class="math inline">\(s\)</span> is expressed as a weighted average of the values of the process at the vertices of the triangle (<span class="math inline">\(Z_{1}\)</span>, <span class="math inline">\(Z_{2}\)</span> and <span class="math inline">\(Z_{3}\)</span>)and with weights equal to
<span class="math inline">\(T_{1} / T\)</span>, <span class="math inline">\(T_{2} / T\)</span> and <span class="math inline">\(T_{3} / T\)</span> where <span class="math inline">\(T\)</span>denotes the area of the big triangle that contains <span class="math inline">\(s\)</span> and <span class="math inline">\(T_{1}\)</span>, <span class="math inline">\(T_{2}\)</span> and <span class="math inline">\(T_{3}\)</span> are the areas of the subtriangles</p>
<p><span class="math inline">\(Z(\boldsymbol{s}) \approx \frac{T_{1}}{T} Z_{1}+\frac{T_{2}}{T} Z_{2}+\frac{T_{3}}{T} Z_{3}\)</span></p>
<p><img src="images/triangle-mesh-1.png" alt="triangle-mesh-1" />
<code>R-INLA</code> provides the <code>inla.spde.make.A()</code> function to easily construct a projection matrix <strong>A</strong>
We create the projection matrix of our example by using <code>inla.spde.make.A()</code> passing the triangulated mesh <code>mesh</code> and the coordinates <code>coo</code>.</p>
<pre><code>A &lt;- inla.spde.make.A(mesh = mesh, loc = coo)</code></pre>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="datapres.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="prd.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": true,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin"],
"google": false
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/NiccoloSalvini/Thesis/edit/master/05-inla_spde.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Niccolo_Salvini_Thesis.pdf", "Niccolo_Salvini_Thesis.epub"],
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"search": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
